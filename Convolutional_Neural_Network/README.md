# Convolutional Neural Network (CNN) from Scratch

## Introduction

A convolutional neural network (CNN) is a particular type of neural network for computer vision tasks. 
It is a powerful tool to process image data. Image data are high-dimensional. 
A typical image for a classification task contains $224\times 224$ RGB values. 
If we use a fully connected neural network with linear layers, we have a huge number of parameters to optimized. 
In addition, nearby image pixels are related. Fully connected neural network cannot treat this relationship. 
Finally, a geometric transformation of image should not change the interpretation of an image. 
A fully connected model must learn all patterns of pixels, which is inefficient. 

A convolutional layer can process each local image region independently, 
using parameters (weights and biases) shared across the whole image. 
It has fewer parameters than the fully connected layer, can understand the spatial relationship, 
and do not need to re-learn the interpretation of the pixels at every position. 
A CNN is a network that predominantly consists convolutional layers. 

In this project, I build a CNN by using **numpy** only. 
The goal of this network is classified the [Fashion MNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist) dataset. 
I learned CNN by this [book](https://udlbook.github.io/udlbook/) and this [article](https://www.quarkml.com/2023/07/build-a-cnn-from-scratch-using-python.html).

## Algorithm
In this work, I built a network that consists of a convolution layer, a pooling layer, flattening layer, and a fully connected (linear) layer, as this image:

![CNN Architecture](https://github.com/zjzhao1002/Machine-Learning-from-Scratch/blob/main/Convolutional_Neural_Network/CNN_Architecutre.png)
*Fig. 1. CNN Architecture ([image source](https://www.quarkml.com/2023/07/build-a-cnn-from-scratch-using-python.html))*

### Convolutional Layer
A convolutional layer is based on the convolutional operation. It is easy to explain it in a 1-Dimension case. 
Assuming the input vector is $\vec{x}$ and the output vector is $\vec{z}$, each component $z_i$ is a weighted sum of nearby inputs:
```math
z_i = \omega_1x_{i-1} + \omega_2x_{i} + \omega_3x_{i+1}
```
The same weights $(\omega_1, \omega_2, omega_3)$ are used at every position and collectively called the convolutional kernel or filter. 
The size of the region over which inputs are combined is termed the kernel size. In previous equation, the kernel size is 3.

![Kernels](https://github.com/zjzhao1002/Machine-Learning-from-Scratch/blob/main/Convolutional_Neural_Network/Conv1a.svg)
*Fig. 2. Kernels in convolutional layer ([image source](https://udlbook.github.io/udlbook/))*

Fig. 2 shows how kernel works. Fig. 2a shows how we deal with the first ouput (where there is no previous input). 
In this case, we can assume the input is zero outside its valid range, which is called padding. 
In the example above, each output is a sum of the nearest three inputs. 
We can also set the *stride*, *kernel size* and *dilation rate* of the kernel. 
Fig. 2a-b shows the *stride=2* case, we only have half the number of outputs. 
Fig. 2c shows the *kernel size=5* case, a larger area is used for convolution. 
Typically, *kernel size* is an odd number so that it can be centered around the current position.
Fig. 2d shows the *dilation rate=2* case. 
Dilation rate is a parameter in the convolution operation that controls how far apart the elements of the kernel (filter) are spaced. 
When the kernel size is large, we have to optimize more weights. In this case we can use dilation rate to reduce parameters, 
because the kernel values are interspersed with zeros. 
For simplicity, *stride=1* and *dilation rate=1* are assumed in my code. 

The Fashion MNIST dataset contains a number of 2D images. 
If $x_{ij}$ represents an element of image and $\omega_{mn}$ is an entry of the kernel, 
The output ($z_{ij}$) can be calculated by 
```math
z_{ij} = \sum^K_{m=1}\sum^K_{n=1}\omega_{mn}x_{i+m-2, j+n-2}
```
where $K$ is the kernel size. We can apply an activation function to introduce nonlinearity, which is ReLU function in my code:
```math
a^C_{ij} = ReLU(z_{ij})
```

If we only apply a single convolution, information is likely be lost. To avoid this, we can use more than one kernel in one layer. 
These kernels perform convolutions parallelly. Each convolution produces a new set of hidden variables, which is called *feature map* or *channel*. 

### Pooling Layer
The pooling layer is implemented to scale down the input. It does this by downsampling the feature maps generated by the convolution layer.

![Pooling](https://github.com/zjzhao1002/Machine-Learning-from-Scratch/blob/main/Convolutional_Neural_Network/ConvDown.svg)
*Fig. 3. Methods for downsampling ([image source](https://udlbook.github.io/udlbook/))*

Fig. 3 shows 3 methods for downsampling, where both dimensions are scaled down by a factor of two: 
1. Sub-sampling (Fig. 3a): We can just sample every other position, which is equivalent to the *stride=2* case.
2. Max pooling (Fig. 3b): The maximum value of the corresponding $2\times 2$ block is retained.
3. Mean pooling (Fig. 3c): Each output is the mean value in the $2\times 2$ block.

Since we use backpropagation to train the network, the gradient corresponding to the sampling values is sent backward. 
In this stage, we have to do upsampling. 

![Pooling](https://github.com/zjzhao1002/Machine-Learning-from-Scratch/blob/main/Convolutional_Neural_Network/ConvUp.svg)
*Fig. 4. Methods for upsampling ([image source](https://udlbook.github.io/udlbook/))*

Fig. 4 shows 3 methods for upsampling:
1. The simplest way is to duplicate each input four times.
2. If we use max pooling previously, we can redistribute the values to their original position, and the rest are set to zero.
3. Bilinear interpolation is used to fill in the missing values between the points we have sampled.

In this work, I use the max pooling.

### Flatten Layer
This layer reshape the pooled feature maps into a one-dimensional vector. 
This step is necessary to connect the output of the previous layers to the fully connected layer. 

In the backpropagation, it reshape the gradient to the shape of the pooling output.

### Fully Connected (Linear) Layer
This layer make the final predictions based on the extracted features. 
It connects every neuron in the previous layer to neurons in this layer, and gives the final output of the network.

### Forward Propagation
The image data are processed with these steps:
1. Convolutional layer (*kernel size=3*, no *biases*, *stride*, and *dilation*)
2. ReLU function
3. Max pooling layer (*pool size=2*)
4. Flatten layer
5. Fully connected layer
6. Softmax function

ReLU function is implemented to introduce nonlinearity. 
Since this is a multi-class classification problem, the activation function for output layer is the softmax function. 

### Backward Propagation
The network is trained by backpropagation method. 
For multi-class classification, the goal of this method is minimizing the loss function (cross entropy loss):
```math
L(a,y) =  -\sum^C_{j=1}y_j\log{a_j}
```
where $a_j$ is the ouput of the $j$ neuron, and $y$ is the vector of target value after one hot encoding. 
As I have done for [neural network](https://github.com/zjzhao1002/Machine-Learning-from-Scratch/tree/main/Neural_Network), 
for the fully connected layer we have:
```math
\begin{split}
\frac{\partial L}{\partial b_j} = \delta^L_j \\
\frac{\partial L}{\partial W_{jk}} = a^{f}_k\delta^L_j
\end{split}
```
where $\delta^L_j = a_j - y_j$ and $a^{f}$ is the output of flatten layer. 
The weights $W$ and biases $b$ of the fully connected layer is updated by 
```math
\begin{split}
W_{jk} \to W_{jk} - \eta\frac{\partial L}{\partial W_{jk}}, \\
b_j \to b_j - \eta\frac{\partial L}{\partial b_j},
\end{split}
```
where $\eta$ is the learning rate.
With this, we can calculate the $\delta$ for previous layer: 
```math
\delta^f = \left( (W^{l+1})^T\delta^{l+1} \right)
```
This $\delta^f$ will be reshaped and upsampled, but values are not changed. 
Assuming after reshape and upsampling, we have a matrix $\delta^p$. 
The derivative of loss function respect to weighted output in the convolutional layer is 
```math
\delta^C = \frac{\partial L}{\partial z_{ij}} = \delta^p \sigma^\prime(z)
```
where $\sigma^\prime$ is the derivative of the ReLU function, $z$ is the weighted output of the convolutional layer. 
Finally, we can calculate the derivative with respect to the kernel weights:
```math
\frac{\partial L}{\partial \omega_{mn}} = \sum_{i}\sum_{j}\frac{\partial L}{\partial z_{ij}}\frac{\partial z_{ij}}{\partial \omega_{mn}}
= \sum_{i}\sum_{j}\frac{\partial L}{\partial z_{ij}} x_{i+m-2, j+n-2}
```
It is another convolutional operation. Finally, we can update the kernel weights:
```math
\omega_{mn} \to \omega_{mn} - \eta\frac{\partial L}{\partial \omega_{mn}}, 
```

### Training and Predictions
Repeat the forward propagation and backpropagation `epochs` times. We should minimize the loss function. 
The kernel weights, weights for linear layer and biases for linear layer should be optimized

After the optimization of the parameters, we can input some data points to the network. 
The feedforward process can use the optimal parameters to give the prediction for each data point.
With the `epochs=10` and `eta=0.01`, the network can reach around 82% accuracy for training dataset and 81% accuracy for test dataset. 
We can say that this network works correctly.

## Conclusion
I have built a CNN from scratch, and learned the key concepts and maths of it. This model works for the MNIST dataset. 
